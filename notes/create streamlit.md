---
date: 2025-12-01
tags:
  -  
  - AI
---

## Diagnosis Extract Model 

```python
!pip install -q streamlit==1.45.1
```
### Initial Code :: 
```python
%%writefile app.py

import streamlit as st

from typing import List

from pydantic import BaseModel

import json

import re

import torch

  

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

import bitsandbytes as bnb

import torch

  

def load_quantized_model(model_name: str,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â load_in_4bit: bool = True,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â use_double_quant: bool = True,

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â quant_type: str = "nf4",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â compute_dtype=torch.float16, Â # use float16 instead of bfloat16 for wider support

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â auth_token: bool = True):

Â  Â  # Clear memory before loading

Â  Â  import gc

Â  Â  gc.collect()

Â  Â  torch.cuda.empty_cache()

  

Â  Â  bnb_config = BitsAndBytesConfig(

Â  Â  Â  Â  load_in_4bit=load_in_4bit,

Â  Â  Â  Â  bnb_4bit_use_double_quant=use_double_quant,

Â  Â  Â  Â  bnb_4bit_quant_type=quant_type,

Â  Â  Â  Â  bnb_4bit_compute_dtype=compute_dtype,

Â  Â  )

  

Â  Â  # ðŸ§  Remove device_map="auto" to avoid meta errors

Â  Â  model = AutoModelForCausalLM.from_pretrained(

Â  Â  Â  Â  model_name,

Â  Â  Â  Â  quantization_config=bnb_config,

Â  Â  Â  Â  torch_dtype=torch.float16,

Â  Â  Â  Â  device_map={"": 0}, Â # send everything to cuda:0 directly

Â  Â  Â  Â  trust_remote_code=True

Â  Â  )

  

Â  Â  tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=auth_token)

Â  Â  tokenizer.pad_token = tokenizer.eos_token

  

Â  Â  return model, tokenizer

  
  

# Replace with your model ID

base_model_id = "CohereForAI/c4ai-command-r7b-arabic-02-2025"

model, tokenizer = load_quantized_model(base_model_id)

  
  

# ========== SCHEMA MODELS ==========

  

class Turn(BaseModel):

Â  Â  role: str

Â  Â  content: str

  

class SummarizationInput(BaseModel):

Â  Â  messages: List[Turn]

  

class TranslateModel(BaseModel):

Â  Â  Translated_conversation: str

  

class SummaryModel(BaseModel):

Â  Â  Diagnosis_summary: str

Â  Â  Patient_symptoms: List[str]

Â  Â  Recommendation: str

  

# ========== FUNCTION: CONVERSATION TO TEXT ==========

  

def convert_conversation_to_text_for_api(conversation: List[Turn]) -> str:

Â  Â  result = []

Â  Â  for turn in conversation:

Â  Â  Â  Â  if turn.role == "assistant":

Â  Â  Â  Â  Â  Â  result.append(f"Doctor: {turn.content.strip()}")

Â  Â  Â  Â  elif turn.role == "user":

Â  Â  Â  Â  Â  Â  result.append(f"Patient: {turn.content.strip()}")

Â  Â  return "\n\n".join(result)

  

# ========== FUNCTION: TRANSLATE TEMPLATE ==========

  

def TranslateTemplate(ar_text: str, translate_model):

Â  Â  return [

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "role": "system",

Â  Â  Â  Â  Â  Â  "content": "\n".join([

Â  Â  Â  Â  Â  Â  Â  Â  "You are a professional translator.",

Â  Â  Â  Â  Â  Â  Â  Â  "You will be provided with an Arabic text.",

Â  Â  Â  Â  Â  Â  Â  Â  "Translate the text into English.",

Â  Â  Â  Â  Â  Â  Â  Â  "Extract JSON details according to the Pydantic schema provided.",

Â  Â  Â  Â  Â  Â  Â  Â  "Make sure that the values in the JSON are written in **English**.",

Â  Â  Â  Â  Â  Â  Â  Â  "Respond ONLY with a valid JSON object that follows the schema â€“ no markdown fences, no extra text."

Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  },

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "role": "user",

Â  Â  Â  Â  Â  Â  "content": "\n".join([

Â  Â  Â  Â  Â  Â  Â  Â  "## conversation",

Â  Â  Â  Â  Â  Â  Â  Â  ar_text,

Â  Â  Â  Â  Â  Â  Â  Â  "",

Â  Â  Â  Â  Â  Â  Â  Â  "## Pydantic Details",

Â  Â  Â  Â  Â  Â  Â  Â  json.dumps(translate_model.model_json_schema(), ensure_ascii=False),

Â  Â  Â  Â  Â  Â  Â  Â  "",

Â  Â  Â  Â  Â  Â  Â  Â  "## Translation : ",

Â  Â  Â  Â  Â  Â  Â  Â  "```json"

Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  }

Â  Â  ]

  

# ========== FUNCTION: TRANSLATE GENERATE ==========

  

def TranslateGenerate(message, tokenizer, model, max_new_tokens=1500):

Â  Â  input_ids = tokenizer.apply_chat_template(

Â  Â  Â  Â  message,

Â  Â  Â  Â  tokenize=True,

Â  Â  Â  Â  add_generation_prompt=True,

Â  Â  Â  Â  return_tensors="pt",

Â  Â  ).to(model.device)

  

Â  Â  gen_tokens = model.generate(

Â  Â  Â  Â  input_ids,

Â  Â  Â  Â  max_new_tokens=max_new_tokens,

Â  Â  Â  Â  do_sample=False,

Â  Â  Â  Â  temperature=None,

Â  Â  )

  

Â  Â  gen_tokens = [

Â  Â  Â  Â  output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids, gen_tokens)

Â  Â  ]

  

Â  Â  gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)

  

Â  Â  match = re.search(r'\{.*?\}', gen_text, re.DOTALL)

Â  Â  if not match:

Â  Â  Â  Â  raise ValueError("No JSON block found in model output")

  

Â  Â  json_block = match.group(0).strip()

Â  Â  return json.loads(json_block)

  

# ========== FUNCTION: SUMMARY TEMPLATE ==========

  

def SummaryTemplate(en_text: str, summary_model):

Â  Â  return [

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "role": "system",

Â  Â  Â  Â  Â  Â  "content": "\n".join([

Â  Â  Â  Â  Â  Â  Â  Â  "You are an NLP data parser.",

Â  Â  Â  Â  Â  Â  Â  Â  "You will be provided by an Arabic text associated with a Pydantic scheme.",

Â  Â  Â  Â  Â  Â  Â  Â  "Generate the output in the same language.",

Â  Â  Â  Â  Â  Â  Â  Â  "Extract JSON details from the text according to the Pydantic details.",

Â  Â  Â  Â  Â  Â  Â  Â  "Do not generate any introduction or conclusion."

Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  },

Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  "role": "user",

Â  Â  Â  Â  Â  Â  "content": "\n".join([

Â  Â  Â  Â  Â  Â  Â  Â  "## conversation",

Â  Â  Â  Â  Â  Â  Â  Â  en_text,

Â  Â  Â  Â  Â  Â  Â  Â  "",

Â  Â  Â  Â  Â  Â  Â  Â  "## Pydantic Details",

Â  Â  Â  Â  Â  Â  Â  Â  json.dumps(summary_model.model_json_schema(), ensure_ascii=False),

Â  Â  Â  Â  Â  Â  Â  Â  "",

Â  Â  Â  Â  Â  Â  Â  Â  "## Summarization : ",

Â  Â  Â  Â  Â  Â  Â  Â  "```json"

Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  }

Â  Â  ]

  

# ========== FUNCTION: SUMMARY GENERATE ==========

  

def SummaryGenerate(message, tokenizer, model, max_new_tokens=200):

Â  Â  input_ids = tokenizer.apply_chat_template(

Â  Â  Â  Â  message,

Â  Â  Â  Â  tokenize=True,

Â  Â  Â  Â  add_generation_prompt=True,

Â  Â  Â  Â  return_tensors="pt",

Â  Â  ).to(model.device)

  

Â  Â  gen_tokens = model.generate(

Â  Â  Â  Â  input_ids,

Â  Â  Â  Â  max_new_tokens=max_new_tokens,

Â  Â  Â  Â  do_sample=False,

Â  Â  Â  Â  temperature=None,

Â  Â  )

  

Â  Â  gen_tokens = [

Â  Â  Â  Â  output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids, gen_tokens)

Â  Â  ]

  

Â  Â  gen_text = tokenizer.decode(gen_tokens[0])

Â  Â  gen_text = gen_text.replace("<|END_RESPONSE|>", "").replace("<|END_OF_TURN_TOKEN|>", "").replace("```", "").strip()

Â  Â  match = re.search(r'\{.*?\}', gen_text, re.DOTALL)

Â  Â  if not match:

Â  Â  Â  Â  raise ValueError("No JSON block found in model output")

  

Â  Â  json_block = match.group(0).strip()

Â  Â  return json.loads(json_block)

  

# ========== STREAMLIT UI ==========

  

st.set_page_config(page_title="Medical Summarizer", layout="wide")

st.title("ðŸ©º Medical Conversation Summarizer")

input_text = st.text_area("Conversation JSON:", height=300)

  

if st.button("Summarize"):

Â  Â  with st.spinner("ðŸ” Running model inference... Please wait..."):

Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  parsed_json = json.loads(input_text)

Â  Â  Â  Â  Â  Â  raw_messages = parsed_json["messages"]

Â  Â  Â  Â  Â  Â  structured_input = SummarizationInput(messages=[Turn(**turn) for turn in raw_messages])

  

Â  Â  Â  Â  Â  Â  ar_text = convert_conversation_to_text_for_api(structured_input.messages)

  

Â  Â  Â  Â  Â  Â  translate_template_message = TranslateTemplate(ar_text, TranslateModel)

Â  Â  Â  Â  Â  Â  translated_json = TranslateGenerate(translate_template_message, tokenizer, model)

Â  Â  Â  Â  Â  Â  en_text = translated_json['Translated_conversation']

  

Â  Â  Â  Â  Â  Â  summary_template_message = SummaryTemplate(en_text, SummaryModel)

Â  Â  Â  Â  Â  Â  summary_json = SummaryGenerate(summary_template_message, tokenizer, model)

  

Â  Â  Â  Â  Â  Â  st.success("âœ… Summary generated successfully!")

Â  Â  Â  Â  Â  Â  st.subheader("ðŸ“‹ Summary Output (JSON):")

Â  Â  Â  Â  Â  Â  st.json(summary_json)

  

Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  st.error(f"âŒ Error: {str(e)}")
```

#### Run streamlit
```python
nest_asyncio.apply()

NGROK_AUTH_TOKEN = userdata.get("NGROK_AUTH_TOKEN")

ngrok.set_auth_token(NGROK_AUTH_TOKEN)

for tunnel in ngrok.get_tunnels():

Â  Â  ngrok.disconnect(tunnel.public_url)

public_url = ngrok.connect(addr=8501)

print("ðŸš€ Your API is live at:", public_url)

!streamlit run app.py &>/content/logs.txt&
```
### 
